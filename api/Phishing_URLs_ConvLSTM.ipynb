{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from string import printable\n",
        "from keras.utils import pad_sequences\n",
        "from pathlib import Path\n",
        "from keras import regularizers, Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Lambda, Flatten, Input, ELU, LSTM, Embedding, BatchNormalization, Conv1D, concatenate, MaxPooling1D\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def read_data():\n",
        "  df = pd.read_csv(\"Phishing_dataset.csv\")\n",
        "  df.drop(df.columns[0], axis=1, inplace=True)\n",
        "  url_int_tokens = [\n",
        "      [printable.index(x) + 1 for x in url if x in printable] for url in df.iloc[:, 0]\n",
        "  ]\n",
        "\n",
        "  max_len = 75\n",
        "  X = pad_sequences(url_int_tokens, maxlen=max_len)\n",
        "  le1 = LabelEncoder()\n",
        "\n",
        "  df['Label'] = le1.fit_transform(df['Label'])\n",
        "  target = np.array(df['Label'])\n",
        "  x_train, x_test, target_train, target_test = train_test_split(X, target, test_size=0.25, random_state=42)\n",
        "\n",
        "  return x_train, x_test, target_train, target_test\n",
        "\n",
        "x_train, x_test, target_train, target_test = read_data()\n",
        "max_len = 75\n",
        "emb_dim = 32\n",
        "max_vocab_len = 101\n",
        "lstm_output_size = 32\n",
        "W_reg = regularizers.l2(1e-4)\n",
        "epochs_num = 10\n",
        "batch_size = 32\n",
        "final_model = Sequential()\n",
        "final_model.add(Embedding(input_dim=max_vocab_len, output_dim=emb_dim, input_length=max_len, embeddings_regularizer=W_reg))\n",
        "final_model.add(Conv1D(kernel_size=5, filters=256, padding='same', activation='elu'))\n",
        "final_model.add(MaxPooling1D(pool_size=4))\n",
        "final_model.add(Dropout(0.5))\n",
        "final_model.add(LSTM(lstm_output_size))\n",
        "final_model.add(Dropout(0.5))\n",
        "final_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "final_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "final_model.fit(x_train, target_train, epochs=epochs_num, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "-ICd6_OaIevL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_model.save('convlstm.h5')\n",
        "print(\"Convolutional LSTM model saved\")\n",
        "\n",
        "convlstm = keras.models.load_model('convlstm.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipQVhCaUJc1O",
        "outputId": "4c1a810c-2681-4aef-c445-a20c988d9ef5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convolutional LSTM model saved\n"
          ]
        }
      ]
    }
  ]
}